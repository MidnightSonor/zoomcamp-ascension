import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import zipfile
import course_code as course
import seaborn as sns
import requests as rq
#from kaggle.api.kaggle_api_extended import KaggleApi
#api = KaggleApi()
#api.authenticate()
#api.dataset_download_file('camnugent/california-housing-prices',
#							file_name = 'housing.csv')
#DATA = "https://www.kaggle.com/datasets/camnugent/california-housing-prices/download"
#DATA = "https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv"

def part_sum(dict_full: dict, index_max):
	if index_max == 0:
		return 1
	else:
		sum_coef = 0
		count = 0
		for key in dict_full.keys():
			sum_coef += dict_full[key]
			count += 1
			if count >= index_max:
				break
		return round(1.0 - sum_coef, 3)


list_of_columns = ['latitude', 'longitude', 'housing_median_age',
				'total_rooms', 'total_bedrooms', 'population',
				'households', 'median_income', 'median_house_value']
#df_raw = pd.read_csv('housing.csv')[list_of_columns]
df_raw = pd.read_csv(zipfile.ZipFile('housing.csv.zip', 'r').open('housing.csv'))[list_of_columns]
#df_raw = pd.read_csv(rq.get(DATA))[list_of_columns]
#df_raw = pd.read_csv(rq.get(DATA).headers)


#print(len(df_raw))
#print(df_raw.isna().sum())
#print(df_raw['population'].quantile(.5))


ratio = {"tr": .8, "val": .0, "test": .2}
data = dict()
target = dict()
deviations = list()

#print(type(ratio.keys()))
#seeds = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
#for seed in seeds:
df = df_raw.sample(frac=1, random_state=9).reset_index(drop=True)
key_ratio = list(ratio.keys())
for i in key_ratio:
	key_num = key_ratio.index(i)
	fraction = ratio[i]/part_sum(ratio, key_num)
	#data[i] = df.sample(frac=fraction).fillna(df.mean())
	data[i] = df.sample(frac=fraction).fillna(0)
	#print(type(data[i]))
	target[i] = np.log1p(data[i]["median_house_value"].values)
	df = df.drop(data[i].index)
	data[i] = data[i].drop("median_house_value", axis=1).values
	print(type(data[i]))
	#print(data['test'])
	#print(target)
	#r_array = [0, 0.000001, 0.0001, 0.001, 0.01, 0.1, 1, 5, 10]
	#for r in r_array:
	#w = course.train_linear_regression_reg(data["tr"], target["tr"], r)
	#print(list(data.keys()))
#print(type(data["tr"]), type(data["val"]))
#data_comb = np.concatenate(np.array(data["tr"]), np.array(data["val"]))
#target_comb = np.concatenate(target["tr"], target["val"])
w = course.train_linear_regression(data["tr"], target["tr"])
prediction = dict()
	#print(w)
prediction["tr"] = w[0] + data["tr"].dot(np.delete(w, 0))
	#deviations.append(round(course.rmse(target["val"], prediction["val"]), 2))
print(course.rmse(target["tr"], prediction["tr"]))
#print(round(np.std(np.array(deviations)), 3))
#plt.figure(figsize=(6, 4))

#sns.histplot(target["tr"], label='target', color='#222222', alpha=0.6, bins=40)
#sns.histplot(prediction["tr"], label='prediction', color='#aaaaaa', alpha=0.8, bins=40)

#plt.legend()

#plt.ylabel('Frequency')
#plt.xlabel('Log(Price + 1)')
#plt.title('Predictions vs actual distribution')

plt.show()

#median_target = np.log1p(df['median_house_value'])
#df_tr = df.sample(frac=.6).drop(columns='median_house_value')
#df_val = df.drop(df_tr.index).sample(frac=.5).drop(columns='median_house_value')
#df_test = df.drop(df_tr.index.join(df_val.index)).drop(columns='median_house_value')
#print(df_tr.dtypes)


#df_raw['median_house_value'].plot.hist()
#plt.show()
#df_raw.isna().sum().sum()

#COURSE FILE STARTS HERE

import numpy as np

def train_linear_regression(X, y):
	X = np.column_stack([np.ones(X.shape[0]), X])
	return np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)


def train_linear_regression_reg(X, y, r=0.0):
	X = np.column_stack([np.ones(X.shape[0]), X])
	return np.linalg.inv(X.T.dot(X) + r * np.eye(X.T.dot(X).shape[0])).dot(X.T).dot(y)

def rmse(y, y_prep):
	return np.sqrt(((y - y_prep)**2).mean())

def prepare_X(df, base: list):
    df_num = df[base]
    df_num = df_num.fillna(0)
    X = df_num.values
    return X

def prepare_X(df, base: list):
    df_num = df[base]
    df_num = df_num.fillna(0)
    X = df_num.values
    return X
